# -*- coding: utf-8 -*-
"""pollutionDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rvjlin4JhiOyN0eZ9CueMUBT_VffhauR
"""

!pip install osmnx
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import zipfile
import io
from datetime import datetime
import time
import warnings
warnings.filterwarnings('ignore')
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut, GeocoderServiceError
import osmnx as ox
import geopandas as gpd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler

import pandas as pd
import requests
import zipfile
import io
import os

def download_and_load_epa_data(year=2023):

    url = f"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_{year}.zip"
    csv_filename = f"daily_aqi_by_county_{year}.csv"
    filepath = f"/content/{csv_filename}"

    if os.path.exists(filepath):
        print(f"File already exists at {filepath}. Loading...")
        df = pd.read_csv(filepath)
        print(f"Loaded {len(df)} records from existing file")
        return df

    try:
        print(f"Downloading from {url}...")
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_file:
            zip_file.extractall("/content/")
            print(f"Extracted to /content/")

        df = pd.read_csv(filepath)
        print(f"Successfully loaded {len(df)} records")
        return df

    except requests.exceptions.RequestException as e:
        print(f"Error downloading file: {e}")
        return None
    except zipfile.BadZipFile:
        print("Error: Downloaded file is not a valid zip file")
        return None
    except FileNotFoundError:
        print(f"Error: CSV file not found in zip archive")
        return None
    except Exception as e:
        print(f"Unexpected error: {e}")
        return None

# Load the data
df_aqi = download_and_load_epa_data(2023)

if df_aqi is not None:
    print(f"\nTotal records: {len(df_aqi)}")

#My tests to make sure it all looks okay
print("First 5 rows:")
display(df_aqi.head())

print("\nAvailable columns:")
print(df_aqi.columns.tolist())

print("\nAQI Statistics:")
print(df_aqi['AQI'].describe())

print(f"\nStates available: {df_aqi['State Name'].nunique()}")
print("\nTop 10 states by record count:")
print(df_aqi['State Name'].value_counts().head(10))

TARGET_STATE = "California"
TARGET_COUNTIES = ["Los Angeles", "San Francisco", "San Diego"]

#Filtering data
if TARGET_COUNTIES:
    df_filtered = df_aqi[
        (df_aqi['State Name'] == TARGET_STATE) &
        (df_aqi['county Name'].isin(TARGET_COUNTIES))
    ]
else:
    df_filtered = df_aqi[df_aqi['State Name'] == TARGET_STATE]

print(f"Filtered to {len(df_filtered)} records")
print(f"Counties included: {sorted(df_filtered['county Name'].unique())}")
print(f"Date range: {df_filtered['Date'].min()} to {df_filtered['Date'].max()}")

def geocode_county(state, county, retry=3):
    geolocator = Nominatim(user_agent="air_quality_project_v1", timeout=10)
    queries = [
        f"{county} County, {state}, USA",
        f"{county}, {state}, USA",
        f"{county} County, {state}"
    ]

    for query in queries:
        for attempt in range(retry):
            try:
                location = geolocator.geocode(query)
                if location:
                    return location.latitude, location.longitude
                time.sleep(1)
            #To make sure theres no erros with any of the geolocation cause that messes up the whole thing.
            except (GeocoderTimedOut, GeocoderServiceError) as e:
                print(f"Attempt {attempt+1} failed for {query}: {str(e)}")
                time.sleep(2)
                continue
            except Exception as e:
                print(f"Error geocoding {query}: {str(e)}")
                break
    return None, None

unique_locations = df_filtered[['State Name', 'county Name']].drop_duplicates().reset_index(drop=True)

print(f"Geocoding {len(unique_locations)} counties")

coordinates = []

for idx, row in unique_locations.iterrows():
    state = row['State Name']
    county = row['county Name']

    print(f"[{idx+1}/{len(unique_locations)}] Geocoding {county}, {state}...", end='')

    lat, lon = geocode_county(state, county)

    if lat and lon:
        print(f"({lat:.4f}, {lon:.4f})")
    else:
        print(f"Failed")

    coordinates.append({
        'State Name': state,
        'county Name': county,
        'Latitude': lat,
        'Longitude': lon
    })
    time.sleep(1.5)

coords_df = pd.DataFrame(coordinates)

# Checking results
successful = coords_df['Latitude'].notna().sum()
failed = coords_df['Latitude'].isna().sum()

print(f"\n Successfully geocoded: {successful}/{len(coords_df)} counties")
if failed > 0:
    print(f" Failed to geocode: {failed} counties")
    print("\nFailed counties:")
    print(coords_df[coords_df['Latitude'].isna()][['State Name', 'county Name']])
coords_df_clean = coords_df.dropna()

print(f"\n Final locations with coordinates: {len(coords_df_clean)}")
display(coords_df_clean)

#Save coordinates
coords_df_clean.to_csv('county_coordinates.csv', index=False)
print("\n Saved coordinates to: county_coordinates.csv")

# Merge coordinates with filtered AQI data
df_with_coords = df_filtered.merge(
    coords_df_clean,
    on=['State Name', 'county Name'],
    how='inner'
)

print(f"Merged dataset: {len(df_with_coords)} records")
print(f"Original records: {len(df_filtered)}")
print(f"Records with coordinates: {len(df_with_coords)}")
print(f"Loss: {len(df_filtered) - len(df_with_coords)} records ({(1 - len(df_with_coords)/len(df_filtered))*100:.1f}%)")

display(df_with_coords.head())

locations = df_with_coords[['State Name', 'county Name', 'Latitude', 'Longitude']].drop_duplicates()
print(f"\n Unique county locations: {len(locations)}")
display(locations)

import time

ox.settings.log_console = False
ox.settings.use_cache = True
ox.settings.timeout = 300
ox.settings.useful_tags_way = ox.settings.useful_tags_way + ['building', 'landuse', 'amenity']

def get_urban_features_simple(lat, lon, radius=3000):
    features = {
        'building_count': 0,
        'road_length_km': 0,
        'road_count': 0,
        'major_roads': 0,
        'minor_roads': 0,
        'poi_count': 0,
    }

    point = (lat, lon)

    # 1. Try to get street network
    try:
        G = ox.graph_from_point(
            point,
            dist=radius,
            network_type='all',
            simplify=True
        )

        features['road_count'] = len(G.edges())

        # Calculate total road length
        total_length = 0
        major = 0
        minor = 0

        for u, v, data in G.edges(data=True):
            length = data.get('length', 0)
            total_length += length

            highway = data.get('highway', '')
            if isinstance(highway, list):
                highway = highway[0] if highway else ''

            if highway in ['motorway', 'trunk', 'primary']:
                major += 1
            else:
                minor += 1

        features['road_length_km'] = total_length / 1000
        features['major_roads'] = major
        features['minor_roads'] = minor

        print(f"  ({features['road_count']} roads)")

    except Exception as e:
        print(f"  {str(e)[:40]}")

    time.sleep(1)

    # 2.get buildings
    try:
        tags = {'building': True}
        buildings = ox.features_from_point(point, tags=tags, dist=radius)
        features['building_count'] = len(buildings)
        print(f"  ({features['building_count']} buildings)")

    except Exception as e:
        print(f"  {str(e)[:40]}")

    time.sleep(1)

    # 3.get POIs
    try:
        tags = {'amenity': True, 'shop': True, 'tourism': True}
        pois = ox.features_from_point(point, tags=tags, dist=radius)
        features['poi_count'] = len(pois)
        print(f"  ({features['poi_count']} POIs)")

    except Exception as e:
        print(f" {str(e)[:40]}")

    return features

# Extract urban features for each county
print(f"\n Extracting urban features for {len(locations)} counties...")
print("This will take several minutes..\n")

urban_features_list = []

for idx, row in locations.iterrows():
    lat, lon = row['Latitude'], row['Longitude']
    county = row['county Name']
    state = row['State Name']

    print(f"\n[{idx+1}/{len(locations)}] Processing {county}, {state}...")

    features = get_urban_features_simple(lat, lon, radius=3000)
    features['Latitude'] = lat
    features['Longitude'] = lon
    features['county Name'] = county
    features['State Name'] = state

    urban_features_list.append(features)

    # Rate limiting - be nice to OSM servers
    time.sleep(3)

urban_features_df = pd.DataFrame(urban_features_list)

print(f"Extracted features for {len(urban_features_df)} locations")

# Save urban features
urban_features_df.to_csv('urban_features.csv', index=False)
print(" Saved to: urban_features.csv")

print("\n Urban Features Summary:")
display(urban_features_df)

# Show statistics
print("\n Feature Statistics:")
display(urban_features_df.describe())

# Merge AQI data with urban features
df_merged = df_with_coords.merge(
    urban_features_df,
    on=['State Name', 'county Name', 'Latitude', 'Longitude'],
    how='left'
)

print(f"Merged dataset: {len(df_merged)} records")

df_merged['Date'] = pd.to_datetime(df_merged['Date'])
df_merged['year'] = df_merged['Date'].dt.year
df_merged['month'] = df_merged['Date'].dt.month
df_merged['day_of_week'] = df_merged['Date'].dt.dayofweek
df_merged['day_of_year'] = df_merged['Date'].dt.dayofyear
df_merged['quarter'] = df_merged['Date'].dt.quarter

df_merged['season'] = df_merged['month'].apply(lambda x:
    1 if x in [12, 1, 2] else      # Winter
    2 if x in [3, 4, 5] else       # Spring
    3 if x in [6, 7, 8] else       # Summer
    4                              # Fall
)
df_merged['is_weekend'] = df_merged['day_of_week'].isin([5, 6]).astype(int)
print("\n Dataset columns:")
print(df_merged.columns.tolist())
# Save complete dataset
df_merged.to_csv('air_quality_complete.csv', index=False)
print("\nSaved complete dataset to: air_quality_complete.csv")

display(df_merged.head(10))

# Summary statistics
print("\nDataset Summary:")
print(f"Total records: {len(df_merged)}")
print(f"Date range: {df_merged['Date'].min()} to {df_merged['Date'].max()}")
print(f"Counties: {df_merged['county Name'].nunique()}")
print(f"Average AQI: {df_merged['AQI'].mean():.2f}")
print(f"AQI range: {df_merged['AQI'].min()} - {df_merged['AQI'].max()}")

#Make all my charts
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

fig, axes = plt.subplots(3, 3, figsize=(18, 14))

# 1. AQI distribution
axes[0, 0].hist(df_merged['AQI'], bins=50, edgecolor='black', color='steelblue', alpha=0.7)
axes[0, 0].set_title('AQI Distribution', fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('AQI')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].axvline(df_merged['AQI'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df_merged["AQI"].mean():.1f}')
axes[0, 0].legend()

# 2. AQI over time
df_merged.groupby('Date')['AQI'].mean().plot(ax=axes[0, 1], color='green', linewidth=1.5)
axes[0, 1].set_title('Average AQI Over Time', fontsize=12, fontweight='bold')
axes[0, 1].set_xlabel('Date')
axes[0, 1].set_ylabel('AQI')
axes[0, 1].grid(True, alpha=0.3)

# 3. AQI by county
county_aqi = df_merged.groupby('county Name')['AQI'].mean().sort_values()
county_aqi.plot(kind='barh', ax=axes[0, 2], color='coral')
axes[0, 2].set_title('Average AQI by County', fontsize=12, fontweight='bold')
axes[0, 2].set_xlabel('Average AQI')
axes[0, 2].set_ylabel('County')

# 4. AQI vs Building Count (log scale)
axes[1, 0].scatter(df_merged['building_count'], df_merged['AQI'], alpha=0.3, s=10, color='purple')
axes[1, 0].set_xscale('log')
axes[1, 0].set_title('AQI vs Building Count (log scale)', fontsize=12, fontweight='bold')
axes[1, 0].set_xlabel('Building Count (log)')
axes[1, 0].set_ylabel('AQI')
axes[1, 0].grid(True, alpha=0.3)


# 5. AQI by season
season_labels = ['Winter', 'Spring', 'Summer', 'Fall']
df_merged.boxplot(column='AQI', by='season', ax=axes[1, 1])
axes[1, 1].set_title('AQI by Season', fontsize=12, fontweight='bold')
axes[1, 1].set_xlabel('Season')
axes[1, 1].set_ylabel('AQI')
plt.sca(axes[1, 1])
plt.xticks([1, 2, 3, 4], season_labels)

# 6. AQI vs Road Length (log scale)
axes[1, 2].scatter(df_merged['road_length_km'], df_merged['AQI'], alpha=0.3, s=10, color='brown')
axes[1, 2].set_xscale('log')
axes[1, 2].set_title('AQI vs Road Length (log scale)', fontsize=12, fontweight='bold')
axes[1, 2].set_xlabel('Road Length (km, log)')
axes[1, 2].set_ylabel('AQI')
axes[1, 2].grid(True, alpha=0.3)

# 7. AQI by month
monthly_aqi = df_merged.groupby('month')['AQI'].mean()
axes[2, 0].plot(monthly_aqi.index, monthly_aqi.values, marker='o', linewidth=2, markersize=8, color='darkblue')
axes[2, 0].set_title('Average AQI by Month', fontsize=12, fontweight='bold')
axes[2, 0].set_xlabel('Month')
axes[2, 0].set_ylabel('Average AQI')
axes[2, 0].set_xticks(range(1, 13))
axes[2, 0].grid(True, alpha=0.3)

# 8. AQI vs Major Roads (log scale)
axes[2, 1].scatter(df_merged['major_roads'], df_merged['AQI'], alpha=0.3, s=10, color='red')
axes[2, 1].set_xscale('log')
axes[2, 1].set_title('AQI vs Major Roads (log scale)', fontsize=12, fontweight='bold')
axes[2, 1].set_xlabel('Major Roads (log)')
axes[2, 1].set_ylabel('AQI')
axes[2, 1].grid(True, alpha=0.3)

# 9. AQI vs POI Count (log scale)
axes[2, 2].scatter(df_merged['poi_count'], df_merged['AQI'], alpha=0.3, s=10, color='orange')
axes[2, 2].set_xscale('log')
axes[2, 2].set_title('AQI vs Points of Interest (log scale)', fontsize=12, fontweight='bold')
axes[2, 2].set_xlabel('POI Count (log)')
axes[2, 2].set_ylabel('AQI')
axes[2, 2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n Correlation with AQI:")
numeric_cols = df_merged.select_dtypes(include=[np.number]).columns
correlation = df_merged[numeric_cols].corr()['AQI'].sort_values(ascending=False)
print(correlation)

#correlation matrix
plt.figure(figsize=(12, 10))
key_features = ['AQI', 'building_count', 'road_length_km', 'road_count',
                'major_roads', 'minor_roads', 'poi_count', 'month',
                'day_of_week', 'season', 'is_weekend']
corr_matrix = df_merged[key_features].corr()

sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

feature_columns = [
    # Urban features
    'building_count',
    'road_length_km',
    'road_count',
    'major_roads',
    'minor_roads',
    'poi_count',
    # Time based features
    'month',
    'day_of_week',
    'season',
    'day_of_year',
    'quarter',
    'is_weekend',
    # Geographic features
    'Latitude',
    'Longitude'
]

df_model = df_merged[feature_columns + ['AQI']].dropna()
print("\nFeature columns:")
for col in feature_columns:
    print(f"  - {col}")
X = df_model[feature_columns]
y = df_model['AQI']
print(f"\nTarget variable (AQI) statistics:")
print(y.describe())
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)
print(f"\n Data Split:")
print(f"  Train set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)")
print(f"  Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)
print("\n Features scaled using StandardScaler")
print("\n Sample of scaled training data:")
display(X_train_scaled_df.head())

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

#Initialize  models
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=10),
    'Random Forest': RandomForestRegressor(
        n_estimators=100,
        random_state=42,
        max_depth=15,
        min_samples_split=5,
        n_jobs=-1
    ),
    'Gradient Boosting': GradientBoostingRegressor(
        n_estimators=100,
        random_state=42,
        max_depth=5,
        learning_rate=0.1
    )
}
results = {}

for name, model in models.items():
    # Train
    start_time = time.time()
    model.fit(X_train_scaled, y_train)
    training_time = time.time() - start_time
    # Predict
    y_pred_train = model.predict(X_train_scaled)
    y_pred_test = model.predict(X_test_scaled)
    # Evaluate
    train_r2 = r2_score(y_train, y_pred_train)
    test_r2 = r2_score(y_test, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
    test_mae = mean_absolute_error(y_test, y_pred_test)
    # Store results
    results[name] = {
        'model': model,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'rmse': test_rmse,
        'mae': test_mae,
        'predictions': y_pred_test,
        'training_time': training_time
    }

    print(f"Training completed in {training_time:.2f} seconds")
    print(f"Train R²: {train_r2:.4f}")
    print(f"Test R²: {test_r2:.4f}")
    print(f"RMSE: {test_rmse:.4f}")
    print(f"MAE: {test_mae:.4f}")
    print("-"*70)

# Create comparison DataFrame
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Train R²': [results[m]['train_r2'] for m in results.keys()],
    'Test R²': [results[m]['test_r2'] for m in results.keys()],
    'RMSE': [results[m]['rmse'] for m in results.keys()],
    'MAE': [results[m]['mae'] for m in results.keys()],
    'Training Time (s)': [results[m]['training_time'] for m in results.keys()]
}).sort_values('Test R²', ascending=False)

print("\n Model Comparison:")
display(comparison_df)

# Select best model
best_model_name = max(results, key=lambda x: results[x]['test_r2'])
best_model = results[best_model_name]['model']

print(f"\n Best Model: {best_model_name}")
print(f"   Test R²: {results[best_model_name]['test_r2']:.4f}")
print(f"   RMSE: {results[best_model_name]['rmse']:.4f}")
print(f"   MAE: {results[best_model_name]['mae']:.4f}")

# Visualize model comparison
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# R² comparison
comparison_df.plot(x='Model', y=['Train R²', 'Test R²'], kind='bar', ax=axes[0], rot=45)
axes[0].set_title('R² Score Comparison', fontsize=12, fontweight='bold')
axes[0].set_ylabel('R² Score')
axes[0].legend(['Train R²', 'Test R²'])
axes[0].grid(True, alpha=0.3)

# Error metrics comparison
comparison_df.plot(x='Model', y=['RMSE', 'MAE'], kind='bar', ax=axes[1], rot=45, color=['red', 'orange'])
axes[1].set_title('Error Metrics Comparison', fontsize=12, fontweight='bold')
axes[1].set_ylabel('Error')
axes[1].legend(['RMSE', 'MAE'])
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Feature importance
print("Feature Importance Analysis\n")

for model_name in ['Decision Tree', 'Random Forest', 'Gradient Boosting']:
    if model_name in results:
        model = results[model_name]['model']

        feature_importance = pd.DataFrame({
            'feature': feature_columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)

        print(f"\n{model_name} - Top 10 Features:")
        display(feature_importance.head(10))

        plt.figure(figsize=(10, 6))
        top_features = feature_importance.head(10)
        plt.barh(top_features['feature'], top_features['importance'])
        plt.xlabel('Importance', fontsize=12)
        plt.title(f'Feature Importance ({model_name})', fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

# Detailed prediction analysis for best model
best_predictions = results[best_model_name]['predictions']
fig, axes = plt.subplots(2, 2, figsize=(14, 12))

#1. Actual vs Predicted scatter plot
axes[0, 0].scatter(y_test, best_predictions, alpha=0.5, s=20)
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
                'r--', lw=2, label='Perfect Prediction')
axes[0, 0].set_xlabel('Actual AQI', fontsize=12)
axes[0, 0].set_ylabel('Predicted AQI', fontsize=12)
axes[0, 0].set_title(f'Actual vs Predicted AQI ({best_model_name})',
                     fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

#Add R² annotation
r2_text = f'R² = {results[best_model_name]["test_r2"]:.4f}'
axes[0, 0].text(0.05, 0.95, r2_text, transform=axes[0, 0].transAxes,
                fontsize=12, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

#2. Residual plot
residuals = y_test - best_predictions
axes[0, 1].scatter(best_predictions, residuals, alpha=0.5, s=20)
axes[0, 1].axhline(y=0, color='r', linestyle='--', linewidth=2)
axes[0, 1].set_xlabel('Predicted AQI', fontsize=12)
axes[0, 1].set_ylabel('Residuals', fontsize=12)
axes[0, 1].set_title('Residual Plot', fontsize=12, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3)

#3. Residual distribution
axes[1, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
axes[1, 0].axvline(x=0, color='r', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('Residuals', fontsize=12)
axes[1, 0].set_ylabel('Frequency', fontsize=12)
axes[1, 0].set_title('Residual Distribution', fontsize=12, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

# 24-Hour AQI Prediction
from datetime import datetime, timedelta

def create_time_series_features(df):
    df = df.sort_values(['county Name', 'Date']).reset_index(drop=True)

    for county in df['county Name'].unique():
        county_mask = df['county Name'] == county

        df.loc[county_mask, 'aqi_lag_1'] = df.loc[county_mask, 'AQI'].shift(1)
        df.loc[county_mask, 'aqi_lag_2'] = df.loc[county_mask, 'AQI'].shift(2)
        df.loc[county_mask, 'aqi_lag_3'] = df.loc[county_mask, 'AQI'].shift(3)
        df.loc[county_mask, 'aqi_lag_7'] = df.loc[county_mask, 'AQI'].shift(7)

        df.loc[county_mask, 'aqi_rolling_mean_3'] = df.loc[county_mask, 'AQI'].rolling(window=3, min_periods=1).mean()
        df.loc[county_mask, 'aqi_rolling_mean_7'] = df.loc[county_mask, 'AQI'].rolling(window=7, min_periods=1).mean()
        df.loc[county_mask, 'aqi_rolling_std_7'] = df.loc[county_mask, 'AQI'].rolling(window=7, min_periods=1).std()
        df.loc[county_mask, 'aqi_trend'] = df.loc[county_mask, 'AQI'] - df.loc[county_mask, 'aqi_rolling_mean_7']

    return df

print("Creating time series features...")
df_ts = create_time_series_features(df_merged.copy())

print("\nNew time series features created:")
print(df_ts[['Date', 'county Name', 'AQI', 'aqi_lag_1', 'aqi_lag_2',
             'aqi_rolling_mean_3', 'aqi_rolling_mean_7']].head(10))

#This is basically setting the lags for the time series
ts_feature_columns = feature_columns + [
    'aqi_lag_1',
    'aqi_lag_2',
    'aqi_lag_3',
    'aqi_lag_7',
    'aqi_rolling_mean_3',
    'aqi_rolling_mean_7',
    'aqi_rolling_std_7',
    'aqi_trend'
]

# remove rows numbers that dont exist
df_ts_clean = df_ts.dropna(subset=ts_feature_columns + ['AQI'])

print(f"\nDataset after adding time series features:")
print(f"Original records: {len(df_ts)}")
print(f"Records with complete features: {len(df_ts_clean)}")
print(f"Records dropped: {len(df_ts) - len(df_ts_clean)}")

df_ts_clean = df_ts_clean.sort_values('Date')
split_date = df_ts_clean['Date'].quantile(0.8)

train_mask = df_ts_clean['Date'] < split_date
test_mask = df_ts_clean['Date'] >= split_date

X_ts_train = df_ts_clean.loc[train_mask, ts_feature_columns]
y_ts_train = df_ts_clean.loc[train_mask, 'AQI']
X_ts_test = df_ts_clean.loc[test_mask, ts_feature_columns]
y_ts_test = df_ts_clean.loc[test_mask, 'AQI']

print(f"\nTemporal split:")
print(f"Train period: {df_ts_clean.loc[train_mask, 'Date'].min()} to {df_ts_clean.loc[train_mask, 'Date'].max()}")
print(f"Test period: {df_ts_clean.loc[test_mask, 'Date'].min()} to {df_ts_clean.loc[test_mask, 'Date'].max()}")
print(f"Train samples: {len(X_ts_train)}")
print(f"Test samples: {len(X_ts_test)}")

# Scale features
from sklearn.preprocessing import StandardScaler

scaler_ts = StandardScaler()
X_ts_train_scaled = scaler_ts.fit_transform(X_ts_train)
X_ts_test_scaled = scaler_ts.transform(X_ts_test)

# rain Time Series Prediction Model

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

print("\n" + "="*70)
print("Training 24-Hour Prediction Models")
print("="*70)

# Train models with time series features
ts_models = {
    'Random Forest (TS)': RandomForestRegressor(
        n_estimators=200,
        random_state=42,
        max_depth=20,
        min_samples_split=5,
        n_jobs=-1
    ),
    'Gradient Boosting (TS)': GradientBoostingRegressor(
        n_estimators=200,
        random_state=42,
        max_depth=7,
        learning_rate=0.05
    )
}

ts_results = {}

for name, model in ts_models.items():
    model.fit(X_ts_train_scaled, y_ts_train)

    y_pred_train = model.predict(X_ts_train_scaled)
    y_pred_test = model.predict(X_ts_test_scaled)

    train_r2 = r2_score(y_ts_train, y_pred_train)
    test_r2 = r2_score(y_ts_test, y_pred_test)
    test_rmse = np.sqrt(mean_squared_error(y_ts_test, y_pred_test))
    test_mae = mean_absolute_error(y_ts_test, y_pred_test)

    ts_results[name] = {
        'model': model,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'rmse': test_rmse,
        'mae': test_mae,
        'predictions': y_pred_test
    }

    print(f"  Train R²: {train_r2:.4f}")
    print(f"  Test R²: {test_r2:.4f}")
    print(f"  RMSE: {test_rmse:.4f}")
    print(f"  MAE: {test_mae:.4f}")

# Select best time series model
best_ts_model_name = max(ts_results, key=lambda x: ts_results[x]['test_r2'])
best_ts_model = ts_results[best_ts_model_name]['model']

print(f"\n{'='*70}")
print(f"Best 24-Hour Prediction Model: {best_ts_model_name}")
print(f"  Test R²: {ts_results[best_ts_model_name]['test_r2']:.4f}")
print(f"  RMSE: {ts_results[best_ts_model_name]['rmse']:.4f}")
print(f"  MAE: {ts_results[best_ts_model_name]['mae']:.4f}")
print(f"{'='*70}")

#Feature Importance for Time Series Model

feature_importance_ts = pd.DataFrame({
    'feature': ts_feature_columns,
    'importance': best_ts_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 15 Most Important Features for 24-Hour Prediction:")
print(feature_importance_ts.head(15))

plt.figure(figsize=(12, 8))
top_15 = feature_importance_ts.head(15)
plt.barh(top_15['feature'], top_15['importance'], color='steelblue')
plt.xlabel('Importance', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.title(f'Feature Importance - 24-Hour AQI Prediction ({best_ts_model_name})',
          fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

#Visualize 24-Hour Predictions

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Actual vs Predicted
best_ts_predictions = ts_results[best_ts_model_name]['predictions']
axes[0, 0].scatter(y_ts_test, best_ts_predictions, alpha=0.5, s=20, color='dodgerblue')
axes[0, 0].plot([y_ts_test.min(), y_ts_test.max()],
                [y_ts_test.min(), y_ts_test.max()],
                'r--', lw=2, label='Perfect Prediction')
axes[0, 0].set_xlabel('Actual AQI (Next Day)', fontsize=12)
axes[0, 0].set_ylabel('Predicted AQI (Next Day)', fontsize=12)
axes[0, 0].set_title('24-Hour Prediction: Actual vs Predicted',
                     fontsize=13, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

r2_text = f'R² = {ts_results[best_ts_model_name]["test_r2"]:.4f}\nRMSE = {ts_results[best_ts_model_name]["rmse"]:.2f}'
axes[0, 0].text(0.05, 0.95, r2_text, transform=axes[0, 0].transAxes,
                fontsize=11, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))

# 2. Time series plot for a specific county
sample_county = df_ts_clean['county Name'].iloc[0]
county_data = df_ts_clean[df_ts_clean['county Name'] == sample_county].copy()
county_test = county_data[county_data['Date'] >= split_date].copy()

if len(county_test) > 0:
    county_test_idx = county_test.index
    test_indices = df_ts_clean.loc[test_mask].index
    mask = test_indices.isin(county_test_idx)
    county_predictions = best_ts_predictions[mask]

    axes[0, 1].plot(county_test['Date'], county_test['AQI'],
                    label='Actual', linewidth=2, marker='o', markersize=4)
    axes[0, 1].plot(county_test['Date'], county_predictions,
                    label='Predicted (24h ahead)', linewidth=2, marker='s', markersize=4)
    axes[0, 1].set_xlabel('Date', fontsize=12)
    axes[0, 1].set_ylabel('AQI', fontsize=12)
    axes[0, 1].set_title(f'24-Hour Forecast Timeline - {sample_county}',
                        fontsize=13, fontweight='bold')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    axes[0, 1].tick_params(axis='x', rotation=45)

# 3. Error distribution
residuals_ts = y_ts_test - best_ts_predictions
axes[1, 0].hist(residuals_ts, bins=50, edgecolor='black', alpha=0.7, color='coral')
axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('Prediction Error (AQI units)', fontsize=12)
axes[1, 0].set_ylabel('Frequency', fontsize=12)
axes[1, 0].set_title('24-Hour Prediction Error Distribution',
                     fontsize=13, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

error_stats = f'Mean: {residuals_ts.mean():.2f}\nStd: {residuals_ts.std():.2f}'
axes[1, 0].text(0.05, 0.95, error_stats, transform=axes[1, 0].transAxes,
                fontsize=11, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))

# 4. Prediction accuracy by AQI category
aqi_categories = pd.cut(y_ts_test, bins=[0, 50, 100, 150, 200, 300, 500],
                        labels=['Good', 'Moderate', 'Unhealthy (SG)',
                                'Unhealthy', 'Very Unhealthy', 'Hazardous'])
category_mae = []
category_names = []

for cat in aqi_categories.cat.categories:
    mask = aqi_categories == cat
    if mask.sum() > 0:
        mae = mean_absolute_error(y_ts_test[mask], best_ts_predictions[mask])
        category_mae.append(mae)
        category_names.append(f"{cat}\n(n={mask.sum()})")

axes[1, 1].bar(range(len(category_names)), category_mae, color='lightgreen', edgecolor='black')
axes[1, 1].set_xticks(range(len(category_names)))
axes[1, 1].set_xticklabels(category_names, fontsize=10)
axes[1, 1].set_ylabel('Mean Absolute Error', fontsize=12)
axes[1, 1].set_title('Prediction Accuracy by AQI Category',
                     fontsize=13, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

def predict_next_day_aqi(county_name, current_features):
    feature_array = np.array([[current_features.get(f, 0) for f in ts_feature_columns]])
    feature_scaled = scaler_ts.transform(feature_array)
    prediction = best_ts_model.predict(feature_scaled)[0]

    return prediction

recent_data = df_ts_clean.sort_values('Date').groupby('county Name').tail(1)
example_county = recent_data.iloc[0]

print(f"\nCounty: {example_county['county Name']}")
print(f"Current Date: {example_county['Date']}")
print(f"Current AQI: {example_county['AQI']:.1f}")
print(f"Previous Day AQI: {example_county['aqi_lag_1']:.1f}")
print(f"7-Day Average: {example_county['aqi_rolling_mean_7']:.1f}")

current_features = {col: example_county[col] for col in ts_feature_columns}
predicted_aqi = predict_next_day_aqi(example_county['county Name'], current_features)

print(f"\nPredicted AQI for Tomorrow: {predicted_aqi:.1f}")

# AQI category
def get_aqi_category(aqi):
    if aqi <= 50: return "Good (Green)"
    elif aqi <= 100: return "Moderate (Yellow)"
    elif aqi <= 150: return "Unhealthy for Sensitive Groups (Orange)"
    elif aqi <= 200: return "Unhealthy (Red)"
    elif aqi <= 300: return "Very Unhealthy (Purple)"
    else: return "Hazardous (Maroon)"

print(f"Predicted Category: {get_aqi_category(predicted_aqi)}")